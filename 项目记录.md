试验结果不是很好，acc = 0.6 ,rec = 0.2 怀疑是网络结构本身性能不加、

正在实验：普通UNet 结果不加sp attention 进行测试。

实验结果：还是网络基础结构过于简单

实验：

使用Unet网络在头部添加sp attention，连续5帧训练，测试效果还可以，训练集上acc= 0.86,rec= 0.80.

效果弱的原因：训练集只有1700张图片，所以在复杂情况下的测试集中效果不够好。


接下来任务：

扩充数据集，验证添加sp attention的方法下，效果优于非连续不加 spattention。

实验结果：

Unet_nosp:

训练31000次，准确率和查全率大约在0.6左右。可见普通UNet网络可以进行简单检测任务，但是准确率与查全率不高。通过与上面实验对比可见，sp attention方法可行并且有效。

但是，由于包含sp的UNet网络结构是否与这次一样，因为中间修改次数过多导致无法确认。

故， 重新测试Unet_with_sp 网络结构结果。

经过测试：

Unet_with sp 网络，avg_acc = 0.62 ,avg_rec = 0.20。

Unet_without sp 网络，avg_acc =0.33,avg_rec=  0.78。

设计实验对比Enet网络with sp 和whthout sp 情况下的差别

实验结果:

Enet_sp :avg_acc0.604_avg_rec0.5636

Enet_without_sp :avg_acc0.3471_avg_rec0.787

实验结果分析，怀疑可能是因为训练方法有问题，连续帧进行训练的话，后面的结果确实是有影响。


准备新数据集


对包含sp_attention模块的ENet网络进行优化，将sp_attention模块添加到encoder的末端，与之前的sp_attention模块在首部的情况进行对比实验


将sp_attention 模块放在encoder末端试验结果（sp 模块是将上一帧的进行处理）：

avg_acc0.84_avg_rec0.59

准确率提升3%，查全率下降6%。


继续实验：

若是sp_attention 处理本帧图片会是什么样结果。


Enet_sp sp在encoder末尾:acc = 0.79,rec = 0.60

Enet_without_sp:avg_acc0.7756,avg_rec0.4487


实验结果表明，在相同数据集下，相同训练方法，添加sp_attention 准确率提升2%，查全率提升16%。

刚刚和姬鹏飞学长聊了一下，他现在遇到的问题是图片输入和输出尺寸都比较小，acc = 0.5,rec = 0.5左右，我可以尝试下有没有什么方法能提升小图片的精度。

实验：

Enet_sp网络输入400*200，输出200*100的情况下,是否能超过0.5的acc和recall。

实验结果：

400*200的情况下，带阈值的准确率 0.97,查全率0.97

需要测试裁剪后输入256*128，输出128*64的结果

测试结果：

在学长的测试集上：avg_acc0.2392_avg_rec0.0905_onep0.0102
